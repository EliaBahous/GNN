{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AadjR9tEd5br",
    "outputId": "d5614f23-5f62-449c-fd4a-54599db94209"
   },
   "outputs": [],
   "source": [
    "!python3.12 --version\n",
    "!python3.12 -m pip install ipykernel\n",
    "!python3.12 -m pip install --upgrade pip\n",
    "!python3.12 -m pip install matplotlib\n",
    "!python3.12 -m pip uninstall torch torch_scatter -y\n",
    "!python3.12 -m pip uninstall numpy -y\n",
    "!python3.12 -m pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "# !export MACOSX_DEPLOYMENT_TARGET=10.14\n",
    "!python3 -m pip install --no-cache-dir torch-scatter -f https://data.pyg.org/whl/torch-2.8.0+cpu.html\n",
    "!python3 -m pip install torch-geometric\n",
    "!python3 -m pip install numba\n",
    "!python3 -m pip install pykeops\n",
    "!python3 -m pip install numpy==2.0.2\n",
    "!python3 -m pip install ogb==1.3.6 torchdiffeq==0.2.5\n",
    "!python3 -m pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSE7nNchv8gs",
    "outputId": "6222c0c0-ddc2-462d-dd95-9d4f7c541c9b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "\n",
    "import data as dt\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from GNN import GNN\n",
    "from mutual import get_optimizer, train\n",
    "from mutual import test as test_model\n",
    "import random\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(base_path, override_path):\n",
    "    with open(base_path) as f:\n",
    "        base = yaml.safe_load(f)\n",
    "    with open(override_path) as f:\n",
    "        override = yaml.safe_load(f)\n",
    "\n",
    "    base.update(override)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoBfaO0tSodU"
   },
   "outputs": [],
   "source": [
    "def construct_graph(edges, attention=None, threshold=0.01):\n",
    "    if isinstance(edges, torch.Tensor):\n",
    "        edges = edges.cpu().numpy()\n",
    "    if attention is not None:\n",
    "        edges = edges[:, attention > threshold]\n",
    "    edge_list = zip(edges[0], edges[1])\n",
    "    g = nx.Graph(edge_list)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9C9nT-Y7Pt_k"
   },
   "outputs": [],
   "source": [
    "def add_noisy_edges(edge_index, num_nodes, noise_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Add random noisy edges to the graph.\n",
    "    \"\"\"\n",
    "    num_edges = edge_index.shape[1]\n",
    "    num_add = int(noise_ratio * num_edges)  # Number of noisy edges to add\n",
    "\n",
    "    added_edges = []\n",
    "    existing_edges = set(map(tuple, edge_index.t().tolist()))  # Convert edges to a set for lookup\n",
    "\n",
    "    while len(added_edges) < num_add:\n",
    "        # Pick two random nodes\n",
    "        u, v = random.randint(0, num_nodes - 1), random.randint(0, num_nodes - 1)\n",
    "\n",
    "        # Ensure it's a new edge and not a self-loop\n",
    "        if u != v and (u, v) not in existing_edges and (v, u) not in existing_edges:\n",
    "            added_edges.append((u, v))\n",
    "\n",
    "    # Convert to tensor and concatenate\n",
    "    print(f\"Edges added: {len(added_edges)}\")\n",
    "    new_edges = torch.tensor(added_edges, dtype=torch.long).t()\n",
    "    return torch.cat([edge_index, new_edges], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65GeNY5be7ar"
   },
   "outputs": [],
   "source": [
    "def add_labels(feat, labels, idx, num_classes, device):\n",
    "  onehot = torch.zeros([feat.shape[0], num_classes]).to(device)\n",
    "  if idx.dtype == torch.bool:\n",
    "    idx = torch.where(idx)[0]  # convert mask to linear index\n",
    "  onehot[idx, labels.squeeze()[idx]] = 1\n",
    "\n",
    "  return torch.cat([feat, onehot], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYHfKAxOBCEN"
   },
   "outputs": [],
   "source": [
    "def to_edge_set(edge_index):\n",
    "    # Convert [2, N] tensor to a set of sorted tuples (for undirected comparison)\n",
    "    return set(tuple(sorted((int(edge_index[0, i]), int(edge_index[1, i])))) for i in range(edge_index.size(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvMOJ6eXBS0I"
   },
   "outputs": [],
   "source": [
    "def score_edges(z, edge_pairs):\n",
    "    return (z[edge_pairs[0]] * z[edge_pairs[1]]).sum(dim=1)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00pTnzj7qRoh"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, pos_encoding=None, opt=None):  # opt required for runtime polymorphism\n",
    "  model.eval()\n",
    "  feat = data.x\n",
    "  if model.opt['use_labels']:\n",
    "    feat = add_labels(feat, data.y, data.train_mask, model.num_classes, model.device)\n",
    "  logits, accs = model(feat, pos_encoding), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return logits,accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epzKchya3x3z"
   },
   "outputs": [],
   "source": [
    "def draw_graph_after_adding_noise(dataset, added_edges):\n",
    "  # Convert to NetworkX\n",
    "  G = to_networkx(dataset.data)\n",
    "\n",
    "  # Add all nodes to ensure disconnected ones are included\n",
    "  G.add_nodes_from(range(dataset.data.num_nodes))\n",
    "\n",
    "  # Draw base graph\n",
    "  pos = nx.spring_layout(G, seed=42)\n",
    "  # nx.draw(G, pos, node_color=\"lightblue\", node_size=4, edge_color=\"gray\", width=0.5)\n",
    "\n",
    "\n",
    "  nx.draw(G, node_color=\"lightblue\", node_size=4, edge_color=\"gray\", width=0.5)\n",
    "\n",
    "\n",
    "  # Highlight the added (noisy) edges\n",
    "  added_edge_list = list(added_edges)\n",
    "  nx.draw_networkx_edges(\n",
    "      G,\n",
    "      pos,\n",
    "      edgelist=added_edge_list,\n",
    "      edge_color=\"red\",\n",
    "      width=1.5,\n",
    "      style=\"dashed\",\n",
    "      label=\"Noisy Edges\"\n",
    "  )\n",
    "\n",
    "  plt.title(\"Graph with Noisy (Added) Edges in Red\")\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vafbV9GgTZnC"
   },
   "outputs": [],
   "source": [
    "def prepare_data(dataset_name: str = 'Cora', percentage_remove: float = 0.25):\n",
    "  # Step 1: Prepare original edges\n",
    "  dataset = Planetoid(root='data', name=dataset_name)\n",
    "  original_edges = dataset.data.edge_index\n",
    "\n",
    "  # Step 2: Remove a subset of edges\n",
    "  num_edges_to_remove = int(original_edges.size(1) * percentage_remove)\n",
    "  edge_indices = list(range(original_edges.size(1)))\n",
    "  random.shuffle(edge_indices)\n",
    "  removed_edge_indices = edge_indices[:num_edges_to_remove]\n",
    "  removed_edges = original_edges[:, removed_edge_indices]\n",
    "\n",
    "  # Step 3: Keep the remaining edges\n",
    "  remaining_edges = torch.cat(\n",
    "      [original_edges[:, i].unsqueeze(1) for i in edge_indices[num_edges_to_remove:]], dim=1\n",
    "  )\n",
    "\n",
    "  # Step 4: Update the graph for training\n",
    "  remaining_edges_to_verify = remaining_edges.clone()\n",
    "  dataset.data.edge_index = remaining_edges\n",
    "\n",
    "\n",
    "  print(\"Training edges:\", dataset.edge_index.shape)\n",
    "  return dataset, removed_edges, remaining_edges_to_verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIZIvNo_yz1y"
   },
   "outputs": [],
   "source": [
    "def trainModel(opt, dataset):\n",
    "  model, dat = GNN(opt, dataset, device).to(device), dataset.data.to(device)\n",
    "  parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "  optimizer = get_optimizer(opt['optimizer'], parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "  best_time = best_epoch = train_acc = val_acc = test_acc = 0\n",
    "\n",
    "\n",
    "  for epoch in range(1, opt['epoch']):\n",
    "      start_time = time.time()\n",
    "\n",
    "      loss = train(model, optimizer, dat)\n",
    "      tmp_train_acc, tmp_val_acc, tmp_test_acc = test_model(model, dat)\n",
    "      best_time = opt['time']\n",
    "      if tmp_val_acc > val_acc:\n",
    "        best_epoch = epoch\n",
    "        train_acc = tmp_train_acc\n",
    "        val_acc = tmp_val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "        best_time = opt['time']\n",
    "      if not opt['no_early'] and model.odeblock.test_integrator.solver.best_val > val_acc:\n",
    "        best_epoch = epoch\n",
    "        val_acc = model.odeblock.test_integrator.solver.best_val\n",
    "        test_acc = model.odeblock.test_integrator.solver.best_test\n",
    "        train_acc = model.odeblock.test_integrator.solver.best_train\n",
    "        best_time = model.odeblock.test_integrator.solver.best_time\n",
    "      log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Best time: {:.4f}'\n",
    "      # print(log.format(epoch, time.time() - start_time, loss, model.fm.sum, model.bm.sum, train_acc, val_acc, test_acc, best_time))\n",
    "\n",
    "  print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d} and best time {:03f}'.format(val_acc, test_acc,\n",
    "                                                                                                     best_epoch,\n",
    "                                                                                                     best_time))\n",
    "  return model, dat, val_acc, tmp_test_acc, train_acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ShXeAjv-2kf"
   },
   "outputs": [],
   "source": [
    "def generate_and_delete_reconstructed_edges(dataset_name: str='Cora', percentage_remove: float = 0.25):\n",
    "      # delete new predict and removed one\n",
    "    dataset, removed_edges, _ = prepare_data(dataset_name=dataset_name, percentage_remove=percentage_remove)\n",
    "    removed_edges_set = set(tuple(sorted((int(u), int(v)))) for u, v in zip(removed_edges[0], removed_edges[1]))\n",
    "    # dataset.data.edge_index = add_noisy_edges(dataset.data.edge_index, num_nodes=dataset.data.num_nodes, noise_ratio=0.001)\n",
    "    print(\"Deleted edges:\", len(removed_edges_set))\n",
    "    return dataset, removed_edges_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04DwucwkvD_v"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def score_edges(embeddings, edge_index):\n",
    "    u = embeddings[edge_index[0]]\n",
    "    v = embeddings[edge_index[1]]\n",
    "    return F.cosine_similarity(u, v)  # returns score per edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "416z0_BW9tpq"
   },
   "outputs": [],
   "source": [
    "def is_close_enough(embeddings, edge_index, epsilon=0.05):\n",
    "    u = embeddings[edge_index[0]]\n",
    "    v = embeddings[edge_index[1]]\n",
    "    distances = torch.norm(u - v, dim=1)\n",
    "    return distances <= epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Dataset Parameterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"pubmed\" # \"cora\" or \"citeseer\"\n",
    "# opt = load_config(\"configs/base.yaml\", f\"configs/{dataset_name}.yaml\")\n",
    "with open(\"configs/generic.yaml\") as f:\n",
    "    opt = yaml.safe_load(f)\n",
    "opt[\"dataset\"] = \"pubmed\"   # or Pubmed / Citeseer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'Dataset: {opt[\"dataset\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dataset, removed_edges = generate_and_delete_reconstructed_edges(dataset_name=opt[\"dataset\"])\n",
    "G = to_networkx(dataset.data)\n",
    "total_tests = 5  # Initial number of test rounds\n",
    "epsilon = 0.1     # Distance threshold for edge prediction\n",
    "known_edges = to_edge_set(dataset.x)\n",
    "num_samples = 50000\n",
    "all_possible = torch.combinations(torch.arange(G.number_of_nodes()), r=2)\n",
    "perm = torch.randperm(all_possible.size(0))[:num_samples]\n",
    "sampled_pairs = all_possible[perm]\n",
    "candidate_edge_index = sampled_pairs.T\n",
    "\n",
    "best_val_acc = test_acc = best_epoch = 0\n",
    "print(\"Working on dataset:\", opt[\"dataset\"], \"started...\\n\")\n",
    "iteration_results = []\n",
    "for i in range(total_tests):\n",
    "    reconstruction_counter = defaultdict(int)\n",
    "    reconstructed_edges_list = []\n",
    "    new_predicted_counter = defaultdict(int)\n",
    "    dataset, removed_edges = generate_and_delete_reconstructed_edges(dataset_name=opt[\"dataset\"], percentage_remove=0.10)\n",
    "    opt['num_feature'] = dataset.data.x.shape\n",
    "    model, dat, val_acc, tmp_test_acc, train_acc, loss = trainModel(opt, dataset)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "        best_epoch = i\n",
    "    log = 'Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(loss, model.fm.sum, model.bm.sum, train_acc, best_val_acc, test_acc))\n",
    "    print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d}'.format(best_val_acc, test_acc, best_epoch))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        feat = dat.x\n",
    "        feat = add_labels(feat, dat.y, dat.train_mask, model.num_classes, model.device)\n",
    "        node_embeddings = model(feat, None)\n",
    "        # Score all candidate edges (e.g., using cosine or dot-product)\n",
    "        scores = score_edges(node_embeddings, candidate_edge_index)\n",
    "\n",
    "        threshold_value = torch.quantile(scores, 0.90)\n",
    "        high_score_mask = scores >= threshold_value\n",
    "        high_score_edges = candidate_edge_index[:, high_score_mask]\n",
    "\n",
    "        # Distance-based filtering\n",
    "        # epsilon = torch.quantile(distances, 0.2).item()\n",
    "        # close_mask = distances <= epsilon\n",
    "        # predicted_edges = high_score_edges[:, close_mask]\n",
    "        distances = torch.norm(node_embeddings[high_score_edges[0]] - node_embeddings[high_score_edges[1]], dim=1)\n",
    "        epsilon = torch.quantile(distances, 0.30).item()\n",
    "        predicted_edges = high_score_edges[:, distances <= epsilon]\n",
    "\n",
    "\n",
    "        print(\"\\n\\nScore range:\", scores.min().item(), \"->\", scores.max().item())\n",
    "        print(\"Distance range:\", distances.min().item(), \"->\", distances.max().item())\n",
    "        print(\"Total predicted edges:\", predicted_edges.size(1))\n",
    "        # Compare predicted edges to removed edges\n",
    "        for u, v in zip(predicted_edges[0], predicted_edges[1]):\n",
    "            edge = tuple(sorted((int(u), int(v))))\n",
    "            if edge in removed_edges:\n",
    "                reconstruction_counter[edge] += 1\n",
    "                reconstructed_edges_list.append(edge)\n",
    "            else:\n",
    "                new_predicted_counter[edge] += 1\n",
    "        \n",
    "        if len(reconstruction_counter) and len(new_predicted_counter):\n",
    "            print(f\"\\nRound {i + 1}, total removed reconstructed: {len(reconstruction_counter.keys())}, \"\n",
    "                  f\"total new reconstructed: {len(new_predicted_counter.keys())}\"\n",
    "                  f\"\\nTotal Weak Edges in round: {len(removed_edges)-len(reconstructed_edges_list)}\")\n",
    "        print(\"\\n ******************************************* \\n\")\n",
    "    iteration_results.append({'reconstruction_counter': dict(reconstruction_counter),\n",
    "                              'weak_edges_counter': dict(removed_edges - set(reconstructed_edges_list)),\n",
    "                             'new_predicted_counter': dict(new_predicted_counter),\n",
    "                              'iteration': i + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFDWKsWHHewo"
   },
   "outputs": [],
   "source": [
    "iteration_results[3]['reconstruction_counter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ozw7IQldBkaL"
   },
   "outputs": [],
   "source": [
    "scores = score_edges(node_embeddings, candidate_edge_index)\n",
    "top_k = 100\n",
    "topk_indices = scores.topk(top_k).indices\n",
    "predicted_edges = candidate_edge_index[:, topk_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYuIxgODG54t"
   },
   "outputs": [],
   "source": [
    "# Display the graph of reconstructed edges\n",
    "import plotly.graph_objects as go\n",
    "selected_iteration = 1  # Index of the iteration to visualize\n",
    "reconstruction_counter = iteration_results[selected_iteration]['reconstruction_counter']\n",
    "new_predicted_counter = iteration_results[selected_iteration]['new_predicted_counter']\n",
    "weak_edges_counter = iteration_results[selected_iteration]['weak_edges_counter']\n",
    "new_labels = []\n",
    "new_counts = []\n",
    "recovered_labels = []\n",
    "recovered_counts = []\n",
    "weak_labels = []\n",
    "weak_counts = []\n",
    "\n",
    "def edge_to_label(edge):\n",
    "    \"\"\"Robustly produce a label string for edge which may be tuple, list, tensor or int.\"\"\"\n",
    "    try:\n",
    "        # Try unpacking (works for tuple/list/1D tensors/arrays)\n",
    "        u, v = edge\n",
    "        return f\"{int(u)}-{int(v)}\"\n",
    "    except Exception:\n",
    "        # Fallback: single value or non-subscriptable -> string representation\n",
    "        return str(int(edge)) if (isinstance(edge, (int,)) or (hasattr(edge, \"dtype\") and getattr(edge, \"dtype\").kind in \"iu\")) else str(edge)\n",
    "\n",
    "for edge, count in reconstruction_counter.items():\n",
    "    label = edge_to_label(edge)\n",
    "    recovered_labels.append(label)\n",
    "    recovered_counts.append(count)\n",
    "\n",
    "for edge, count in new_predicted_counter.items():\n",
    "    label = edge_to_label(edge)\n",
    "    new_labels.append(label)\n",
    "    new_counts.append(count)\n",
    "\n",
    "for edge, count in weak_edges_counter.items():\n",
    "    # weak_edges_counter may use single ints as keys; handle both cases\n",
    "    label = edge_to_label(edge)\n",
    "    weak_labels.append(label)\n",
    "    weak_counts.append(count)\n",
    "# Plotly bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=recovered_labels,\n",
    "    y=recovered_counts,\n",
    "    name='Recovered (Originally Removed)',\n",
    "    marker_color='cornflowerblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=new_labels,\n",
    "    y=new_counts,\n",
    "    name='New Predicted (Not Originally Present)',\n",
    "    marker_color='orange'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=weak_labels,\n",
    "    y=weak_counts,\n",
    "    name='Weak Edges (Not Recovered)',\n",
    "    marker_color='lightgrey'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Edge Reconstruction vs New Predictions vs Weak Edges, Total Iteration:\" + str(total_tests),\n",
    "    xaxis_title=\"Edge (u-v)\",\n",
    "    yaxis_title=\"Reconstruction Count\",\n",
    "    barmode='group',\n",
    "    xaxis_tickangle=-45,\n",
    "    template='plotly_white',\n",
    "    margin=dict(l=40, r=40, t=60, b=100)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4yhkTecodkp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(iteration_results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('edge_reconstruction_data.csv', index=False)\n",
    "\n",
    "print(\"Data saved to edge_reconstruction_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvR0R1z2LGP1"
   },
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "# Create PyVis network\n",
    "net = Network(notebook=False, height=\"750px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "\n",
    "# Add graph from networkx\n",
    "net.from_nx(G)\n",
    "\n",
    "# Highlight reconstructed edges in red\n",
    "for u, v in iteration_results[1]['reconstruction_counter'].keys():\n",
    "    net.add_edge(int(u), int(v), color='red', width=10)\n",
    "\n",
    "# Optional: change node size or color\n",
    "for node in G.nodes():\n",
    "    net.get_node(node)['size'] = 10\n",
    "    net.get_node(node)['color'] = \"#cccccc\"\n",
    "\n",
    "# Save and open in browser\n",
    "net.save_graph(\"graph_reconstruction.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of the testing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8NVMyaeZZbC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract values\n",
    "iterations = [entry[\"iteration\"] for entry in iteration_results]\n",
    "# deleted = [entry[\"deleted\"] for entry in iteration_results]\n",
    "reconstructed = [len(entry[\"reconstruction_counter\"]) for entry in iteration_results]\n",
    "new_reconstructed = [len(entry[\"new_predicted_counter\"]) for entry in iteration_results]\n",
    "weak_edges = [len(entry[\"weak_edges_counter\"]) for entry in iteration_results]\n",
    "# Bar chart setup\n",
    "bar_width = 0.25\n",
    "x = np.arange(len(iterations))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Bars\n",
    "# plt.bar(x - bar_width, deleted, width=bar_width, label='Total Deleted')\n",
    "plt.bar(x, reconstructed, width=bar_width, label='Total Reconstructed')\n",
    "plt.bar(x + bar_width, new_reconstructed, width=bar_width, label='New Reconstructed')\n",
    "plt.bar(x + 0.5*bar_width, weak_edges, width=bar_width, label='Weak Edges (Not Reconstructed)')\n",
    "# Labels and ticks\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Number of Edges\")\n",
    "plt.title(\"Edge Reconstruction Metrics Per Iteration :\" + opt[\"dataset\"] + \" total Iterations \" + str(total_tests))\n",
    "plt.xticks(x, [f\"Iter {i}\" for i in iterations])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuARNTWjukwJ"
   },
   "outputs": [],
   "source": [
    "# %80 %20 - accuracy best\n",
    "# %70 %30 - accuracy best\n",
    "# ....\n",
    "# 85% %15 - ....\n",
    "\n",
    "\n",
    "# removed edges -> evaluations\n",
    "# delete_reconstructed....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2b_h-mEpDBe"
   },
   "outputs": [],
   "source": [
    "def get_node_embeddings(model, data):\n",
    "    \"\"\"Extract final node embeddings from the GNN model\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # If use_labels is True, add labels to features before passing to the model\n",
    "        if model.opt['use_labels']:\n",
    "            x = add_labels(data.x, data.y, data.train_mask, model.num_classes, model.device)\n",
    "        else:\n",
    "            x = data.x\n",
    "\n",
    "        # Get model output, handle single or multiple outputs\n",
    "        output = model(x, None)\n",
    "\n",
    "        # If only embeddings are returned, unpack accordingly\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            embeddings = output\n",
    "        # If multiple outputs are returned, unpack as before\n",
    "        else:\n",
    "            embeddings, _ = output\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def calculate_confidence(embeddings, edge_index):\n",
    "    \"\"\"Calculate cosine similarity between connected nodes\"\"\"\n",
    "    src_emb = embeddings[edge_index[0]]\n",
    "    dst_emb = embeddings[edge_index[1]]\n",
    "    confidence = torch.cosine_similarity(src_emb, dst_emb).numpy()\n",
    "    return (edge_index.numpy(), confidence)\n",
    "\n",
    "def draw_confidence_graph(edges, confidence, threshold=0.5):\n",
    "    \"\"\"Draw graph with confidence-colored edges\"\"\"\n",
    "    G = nx.Graph()\n",
    "    edge_list = []\n",
    "\n",
    "    for i in range(edges.shape[1]):\n",
    "        if confidence[i] > threshold:\n",
    "            edge_list.append((edges[0,i], edges[1,i],\n",
    "                           {'confidence': confidence[i]}))\n",
    "\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    edge_colors = [d['confidence'] for _,_,d in G.edges(data=True)]\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    nx.draw(G, pos, node_size=20,\n",
    "            edge_color=edge_colors, edge_cmap=plt.cm.Reds,\n",
    "            width=1.5, arrows=False)\n",
    "    plt.title(\"Reconstructed Edges with Confidence Scores\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45SKdA-Fwh5v"
   },
   "outputs": [],
   "source": [
    "# After training the model\n",
    "embeddings = get_node_embeddings(model, dataset.data)\n",
    "edges, confidence = calculate_confidence(embeddings, dataset.data.edge_index)\n",
    "\n",
    "# Visualize high-confidence edges (threshold=0.7)\n",
    "draw_confidence_graph(edges, confidence, threshold=0.7)\n",
    "\n",
    "# Compare with original graph\n",
    "draw_graph_after_adding_noise(dataset, edges.T)  # Existing visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN-kE5vg3z5_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_node_embeddings(model, data):\n",
    "    \"\"\"Extract final node embeddings from the GNN model\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # If use_labels is True, add labels to features before passing to the model\n",
    "        if model.opt['use_labels']:\n",
    "            x = add_labels(data.x, data.y, data.train_mask, model.num_classes, model.device)\n",
    "        else:\n",
    "            x = data.x\n",
    "\n",
    "        # Get model output, handle single or multiple outputs\n",
    "        output = model(x, None)\n",
    "\n",
    "        # If only embeddings are returned, unpack accordingly\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            embeddings = output\n",
    "        # If multiple outputs are returned, unpack as before\n",
    "        else:\n",
    "            embeddings, _ = output\n",
    "    return embeddings\n",
    "\n",
    "def calculate_confidence(embeddings, edge_index):\n",
    "    \"\"\"Calculate cosine similarity between connected nodes\"\"\"\n",
    "    src_emb = embeddings[edge_index[0]]\n",
    "    dst_emb = embeddings[edge_index[1]]\n",
    "    scores = score_edges(node_embeddings, candidate_edge_index)  # Already used\n",
    "    top_scores, top_indices = scores.topk(100)\n",
    "    predicted_edges = candidate_edge_index[:, top_indices]\n",
    "    confidence = top_scores.cpu().numpy()\n",
    "    return (edge_index.numpy(), confidence)\n",
    "\n",
    "def draw_confidence_graph(edges, confidence, original_edge_index=None, threshold=None, figsize=(12, 16)):\n",
    "    \"\"\"\n",
    "    Draw graph with confidence-colored edges and show a matching histogram\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    edges : numpy.ndarray\n",
    "        Edge list with shape (2, num_edges) for reconstructed edges\n",
    "    confidence : numpy.ndarray\n",
    "        Confidence scores for each edge\n",
    "    original_edge_index : torch.Tensor or numpy.ndarray, optional\n",
    "        The original edge indices from the dataset\n",
    "    threshold : float or None\n",
    "        If provided, shows a line at this threshold in the histogram\n",
    "        and will be used as the minimum value for the colormap\n",
    "    figsize : tuple\n",
    "        Figure size for the plot (width, height)\n",
    "    \"\"\"\n",
    "    # Create figure with stacked subplots - graph on top (larger), histogram below\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize,\n",
    "                                  gridspec_kw={'height_ratios': [3, 1]})  # 3:1 ratio favoring the graph\n",
    "\n",
    "    # Build the graph with ALL confidence values (no filtering)\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Convert original edge indices to set for fast lookup if provided\n",
    "    original_edges_set = set()\n",
    "    if original_edge_index is not None:\n",
    "        if isinstance(original_edge_index, torch.Tensor):\n",
    "            original_edge_index = original_edge_index.numpy()\n",
    "\n",
    "        for i in range(original_edge_index.shape[1]):\n",
    "            # Add both directions (undirected graph)\n",
    "            edge = (original_edge_index[0, i], original_edge_index[1, i])\n",
    "            original_edges_set.add(edge)\n",
    "            original_edges_set.add(edge[::-1])  # Add reverse edge too\n",
    "\n",
    "    # Separate edges into original (existing) and reconstructed\n",
    "    original_edge_tuples = []\n",
    "    reconstructed_edge_tuples = []\n",
    "    reconstructed_confidence = []\n",
    "\n",
    "    if isinstance(edges, torch.Tensor):\n",
    "        edges = edges.numpy()\n",
    "\n",
    "\n",
    "    for i in range(edges.shape[1]):\n",
    "        u, v = edges[0, i], edges[1, i]\n",
    "        current_edge_tuple = tuple(sorted((int(u), int(v)))) # Ensure consistent edge representation\n",
    "\n",
    "        if original_edge_index is not None and current_edge_tuple in original_edges_set:\n",
    "            # This edge exists in the original graph\n",
    "            original_edge_tuples.append((u, v))\n",
    "        else:\n",
    "            # This is a reconstructed or potentially new edge\n",
    "            reconstructed_edge_tuples.append((u, v))\n",
    "            reconstructed_confidence.append(confidence[i])  # Append the confidence score\n",
    "\n",
    "    # Add all unique edges to the graph first\n",
    "    all_unique_edges = list(set(original_edge_tuples + reconstructed_edge_tuples))\n",
    "    G.add_edges_from(all_unique_edges)\n",
    "\n",
    "    # Add attributes to the reconstructed edges\n",
    "    for i, edge in enumerate(reconstructed_edge_tuples):\n",
    "        u, v = edge\n",
    "        G[u][v]['confidence'] = reconstructed_confidence[i]\n",
    "        G[u][v]['original'] = False # Explicitly mark as not original\n",
    "\n",
    "    # Add attribute to original edges (if needed for later use)\n",
    "    for u, v in original_edge_tuples:\n",
    "         # Check if the edge exists before adding attribute\n",
    "         if G.has_edge(u, v):\n",
    "             G[u][v]['original'] = True\n",
    "\n",
    "\n",
    "    # Set up color mapping for reconstructed edges\n",
    "    cmap = plt.cm.viridis  # Using viridis colormap (can be changed to Reds, etc.)\n",
    "\n",
    "    # Set colormap boundaries for reconstructed edges\n",
    "    if reconstructed_confidence:  # Only if we have reconstructed edges\n",
    "        if threshold is not None:\n",
    "            vmin = threshold\n",
    "        else:\n",
    "            vmin = min(reconstructed_confidence)\n",
    "        vmax = max(reconstructed_confidence)\n",
    "    else:\n",
    "        vmin, vmax = 0, 1  # Default if no reconstructed edges\n",
    "\n",
    "    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # Draw the graph on the top subplot (larger area)\n",
    "    pos = nx.spring_layout(G, seed=42)  # Fixed seed for reproducibility\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=2, node_color='gray', ax=ax1)\n",
    "\n",
    "    # Draw original edges in black\n",
    "    original_edges_to_draw = [(u, v) for u, v, d in G.edges(data=True) if d.get('original', False)]\n",
    "    if original_edges_to_draw:\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                             edgelist=original_edges_to_draw,\n",
    "                             edge_color='black',\n",
    "                             width=0.2,\n",
    "                             alpha=0.9,\n",
    "                             ax=ax1,\n",
    "                             label='Original Edges')\n",
    "\n",
    "    # Draw reconstructed edges with confidence colors\n",
    "    reconstructed_edges_to_draw = [(u, v) for u, v, d in G.edges(data=True) if not d.get('original', True) and 'confidence' in d]\n",
    "    if reconstructed_edges_to_draw:\n",
    "        edge_colors = [G[u][v]['confidence'] for u, v in reconstructed_edges_to_draw]\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                             edgelist=reconstructed_edges_to_draw,\n",
    "                             edge_color=edge_colors,\n",
    "                             edge_cmap=cmap,\n",
    "                             edge_vmin=vmin,\n",
    "                             edge_vmax=vmax,\n",
    "                             width=3.5,\n",
    "                             alpha=0.7,\n",
    "                             ax=ax1,\n",
    "                             label='Reconstructed Edges')\n",
    "\n",
    "        # Add a colorbar for edge confidence\n",
    "        sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=ax1, label='Confidence Score', orientation='vertical',\n",
    "                           pad=0.01, fraction=0.05)\n",
    "\n",
    "    ax1.set_title(\"Graph with Original (Black) and Reconstructed (Colored) Edges\", fontsize=14)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Add legend for edge types\n",
    "    # Check if both types of edges were drawn before adding legend\n",
    "    if original_edges_to_draw or reconstructed_edges_to_draw:\n",
    "        from matplotlib.lines import Line2D\n",
    "        legend_elements = []\n",
    "        if original_edges_to_draw:\n",
    "             legend_elements.append(Line2D([0], [0], color='black', lw=4, label='Original Edges'))\n",
    "        if reconstructed_edges_to_draw:\n",
    "             # Using cmap(0.5) gives an intermediate color from the colormap for the legend\n",
    "             legend_elements.append(Line2D([0], [0], color=cmap(0.5 if reconstructed_confidence else 0), lw=3.5, label='Reconstructed Edges'))\n",
    "        if legend_elements:\n",
    "            ax1.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "\n",
    "    # Draw the histogram on the bottom subplot for reconstructed edge confidence\n",
    "    if reconstructed_confidence:\n",
    "        bins = np.linspace(min(reconstructed_confidence), max(reconstructed_confidence), 20)\n",
    "        ax2.hist(reconstructed_confidence, bins=bins, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        ax2.set_title(\"Distribution of Reconstructed Edge Confidence Scores\", fontsize=12)\n",
    "        ax2.set_xlabel(\"Confidence Score\")\n",
    "        ax2.set_ylabel(\"Frequency\")\n",
    "        ax2.grid(alpha=0.3)\n",
    "\n",
    "        # Add vertical line for threshold if provided\n",
    "        if threshold is not None:\n",
    "            ax2.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.2f}') # Format threshold for clarity\n",
    "            ax2.legend()\n",
    "\n",
    "        # Add statistics to the histogram plot\n",
    "        avg_confidence = np.mean(reconstructed_confidence)\n",
    "        median_confidence = np.median(reconstructed_confidence)\n",
    "\n",
    "        # Add statistics to the histogram plot\n",
    "        avg_confidence = np.mean(reconstructed_confidence)\n",
    "        median_confidence = np.median(reconstructed_confidence)\n",
    "\n",
    "        stats_text = (f\"Statistics (Reconstructed Edges):\\n\"\n",
    "                     f\"Mean: {avg_confidence:.3f}\\n\"\n",
    "                     f\"Median: {median_confidence:.3f}\\n\"\n",
    "                     f\"Min: {min(reconstructed_confidence):.3f}\\n\"\n",
    "                     f\"Max: {max(reconstructed_confidence):.3f}\\n\"\n",
    "                     f\"Edges: {len(reconstructed_confidence)}\")\n",
    "\n",
    "        ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"No reconstructed edges to display\",\n",
    "                 transform=ax2.transAxes, ha='center', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0u9TT3Gn3059"
   },
   "outputs": [],
   "source": [
    "# After training the model\n",
    "embeddings = get_node_embeddings(model, dataset.data)\n",
    "edges, confidence = calculate_confidence(embeddings, dataset.data.edge_index)\n",
    "\n",
    "# Visualize with the new function including original edge information\n",
    "fig, (graph_ax, hist_ax) = draw_confidence_graph(\n",
    "    edges,\n",
    "    confidence,\n",
    "    original_edge_index=dataset.data.edge_index,  # Pass original edges\n",
    "    threshold=0.1\n",
    ")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
