{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AadjR9tEd5br",
    "outputId": "d5614f23-5f62-449c-fd4a-54599db94209"
   },
   "outputs": [],
   "source": [
    "!python3.12 --version\n",
    "!python3.12 -m pip install ipykernel\n",
    "!python3.12 -m pip install --upgrade pip\n",
    "!python3.12 -m pip install matplotlib\n",
    "!python3.12 -m pip uninstall torch torch_scatter -y\n",
    "!python3.12 -m pip uninstall numpy -y\n",
    "!python3.12 -m pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "!export MACOSX_DEPLOYMENT_TARGET=10.14\n",
    "!python3 -m pip install --no-cache-dir torch-scatter -f https://data.pyg.org/whl/torch-2.8.0+cpu.html\n",
    "!python3 -m pip install torch-geometric\n",
    "!python3 -m pip install numba\n",
    "!python3 -m pip install pykeops\n",
    "!python3 -m pip install numpy==2.0.2\n",
    "!python3 -m pip install ogb==1.3.6 torchdiffeq==0.2.5\n",
    "!python3 -m pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSE7nNchv8gs",
    "outputId": "6222c0c0-ddc2-462d-dd95-9d4f7c541c9b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "\n",
    "import data as dt\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from GNN import GNN\n",
    "from mutual import get_optimizer, train\n",
    "from mutual import test as test_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for other pc launch this\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aH6gIcdWvcGM"
   },
   "outputs": [],
   "source": [
    "opt = {'self_loop_weight': 1, 'leaky_relu_slope': 0.2, 'heads': 2, 'K': 10, 'not_lcc': True, 'dataset': 'Cora', 'force_reload': True,\n",
    "        'attention_norm_idx': 0, 'simple': True, 'alpha': 0, 'alpha_dim': 'sc', 'beta_dim': 'sc', \"use_labels\": True,\n",
    "        'hidden_dim': 64, 'block': 'rewire_attention', 'function': 'laplacian', 'alpha_sigmoid': True, 'augment': False, 'adjoint': False,\n",
    "        'tol_scale': 70, 'time': 20, 'input_dropout': 0.5, 'dropout': 0.2, 'method': 'dopri5', 'optimizer':'adam', 'lr':0.008, \"use_mlp\": True,\n",
    "        'decay':0.007, 'epoch':20, 'kinetic_energy':None, 'jacobian_norm2':None, 'total_deriv':None, 'directional_penalty':None, \"beltrami\": False}\n",
    "opt[\"batch_norm\"] = False\n",
    "opt[\"heads\"] = 8\n",
    "opt[\"attention_dim\"] = 128\n",
    "opt['attention_type'] = 'scaled_dot'\n",
    "opt['label_rate'] = 0.5\n",
    "opt['square_plus'] = True\n",
    "opt['reweight_attention'] = False\n",
    "opt['step_size'] = 1\n",
    "opt['max_nfe'] = 5000\n",
    "opt['no_alpha_sigmoid'] = False\n",
    "opt['add_source'] = False\n",
    "opt['fc_out'] = False\n",
    "opt['att_samp_pct'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoBfaO0tSodU"
   },
   "outputs": [],
   "source": [
    "def construct_graph(edges, attention=None, threshold=0.01):\n",
    "    if isinstance(edges, torch.Tensor):\n",
    "        edges = edges.cpu().numpy()\n",
    "    if attention is not None:\n",
    "        edges = edges[:, attention > threshold]\n",
    "    edge_list = zip(edges[0], edges[1])\n",
    "    g = nx.Graph(edge_list)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9C9nT-Y7Pt_k"
   },
   "outputs": [],
   "source": [
    "def add_noisy_edges(edge_index, num_nodes, noise_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Add random noisy edges to the graph.\n",
    "    \"\"\"\n",
    "    num_edges = edge_index.shape[1]\n",
    "    num_add = int(noise_ratio * num_edges)  # Number of noisy edges to add\n",
    "\n",
    "    added_edges = []\n",
    "    existing_edges = set(map(tuple, edge_index.t().tolist()))  # Convert edges to a set for lookup\n",
    "\n",
    "    while len(added_edges) < num_add:\n",
    "        # Pick two random nodes\n",
    "        u, v = random.randint(0, num_nodes - 1), random.randint(0, num_nodes - 1)\n",
    "\n",
    "        # Ensure it's a new edge and not a self-loop\n",
    "        if u != v and (u, v) not in existing_edges and (v, u) not in existing_edges:\n",
    "            added_edges.append((u, v))\n",
    "\n",
    "    # Convert to tensor and concatenate\n",
    "    print(f\"Edges added: {len(added_edges)}\")\n",
    "    new_edges = torch.tensor(added_edges, dtype=torch.long).t()\n",
    "    return torch.cat([edge_index, new_edges], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65GeNY5be7ar"
   },
   "outputs": [],
   "source": [
    "def add_labels(feat, labels, idx, num_classes, device):\n",
    "  onehot = torch.zeros([feat.shape[0], num_classes]).to(device)\n",
    "  if idx.dtype == torch.bool:\n",
    "    idx = torch.where(idx)[0]  # convert mask to linear index\n",
    "  onehot[idx, labels.squeeze()[idx]] = 1\n",
    "\n",
    "  return torch.cat([feat, onehot], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYHfKAxOBCEN"
   },
   "outputs": [],
   "source": [
    "def to_edge_set(edge_index):\n",
    "    # Convert [2, N] tensor to a set of sorted tuples (for undirected comparison)\n",
    "    return set(tuple(sorted((int(edge_index[0, i]), int(edge_index[1, i])))) for i in range(edge_index.size(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvMOJ6eXBS0I"
   },
   "outputs": [],
   "source": [
    "def score_edges(z, edge_pairs):\n",
    "    return (z[edge_pairs[0]] * z[edge_pairs[1]]).sum(dim=1)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00pTnzj7qRoh"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, pos_encoding=None, opt=None):  # opt required for runtime polymorphism\n",
    "  model.eval()\n",
    "  feat = data.x\n",
    "  if model.opt['use_labels']:\n",
    "    feat = add_labels(feat, data.y, data.train_mask, model.num_classes, model.device)\n",
    "  logits, accs = model(feat, pos_encoding), []\n",
    "  for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    accs.append(acc)\n",
    "  return logits,accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epzKchya3x3z"
   },
   "outputs": [],
   "source": [
    "def draw_graph_after_adding_noise(dataset, added_edges):\n",
    "  # Convert to NetworkX\n",
    "  G = to_networkx(dataset.data)\n",
    "\n",
    "  # Add all nodes to ensure disconnected ones are included\n",
    "  G.add_nodes_from(range(dataset.data.num_nodes))\n",
    "\n",
    "  # Draw base graph\n",
    "  pos = nx.spring_layout(G, seed=42)\n",
    "  # nx.draw(G, pos, node_color=\"lightblue\", node_size=4, edge_color=\"gray\", width=0.5)\n",
    "\n",
    "\n",
    "  nx.draw(G, node_color=\"lightblue\", node_size=4, edge_color=\"gray\", width=0.5)\n",
    "\n",
    "\n",
    "  # Highlight the added (noisy) edges\n",
    "  added_edge_list = list(added_edges)\n",
    "  nx.draw_networkx_edges(\n",
    "      G,\n",
    "      pos,\n",
    "      edgelist=added_edge_list,\n",
    "      edge_color=\"red\",\n",
    "      width=1.5,\n",
    "      style=\"dashed\",\n",
    "      label=\"Noisy Edges\"\n",
    "  )\n",
    "\n",
    "  plt.title(\"Graph with Noisy (Added) Edges in Red\")\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vafbV9GgTZnC"
   },
   "outputs": [],
   "source": [
    "def prepare_data(dataset_name: str = 'Cora'):\n",
    "  # Step 1: Prepare original edges\n",
    "  dataset = Planetoid(root='data', name=dataset_name)\n",
    "  original_edges = dataset.data.edge_index\n",
    "\n",
    "  # Step 2: Remove a subset of edges\n",
    "  num_edges_to_remove = int(original_edges.size(1) * 0.25)\n",
    "  edge_indices = list(range(original_edges.size(1)))\n",
    "  random.shuffle(edge_indices)\n",
    "  removed_edge_indices = edge_indices[:num_edges_to_remove]\n",
    "  removed_edges = original_edges[:, removed_edge_indices]\n",
    "\n",
    "  # Step 3: Keep the remaining edges\n",
    "  remaining_edges = torch.cat(\n",
    "      [original_edges[:, i].unsqueeze(1) for i in edge_indices[num_edges_to_remove:]], dim=1\n",
    "  )\n",
    "\n",
    "  # Step 4: Update the graph for training\n",
    "  remaining_edges_to_verify = remaining_edges.clone()\n",
    "  dataset.data.edge_index = remaining_edges\n",
    "\n",
    "\n",
    "  print(\"Training edges:\", dataset.edge_index.shape)\n",
    "  return dataset, removed_edges, remaining_edges_to_verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIZIvNo_yz1y"
   },
   "outputs": [],
   "source": [
    "def trainModel(opt, dataset):\n",
    "  model, dat = GNN(opt, dataset, device).to(device), dataset.data.to(device)\n",
    "  parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "  optimizer = get_optimizer(opt['optimizer'], parameters, lr=opt['lr'], weight_decay=opt['decay'])\n",
    "  best_time = best_epoch = train_acc = val_acc = test_acc = 0\n",
    "\n",
    "\n",
    "  for epoch in range(1, opt['epoch']):\n",
    "      start_time = time.time()\n",
    "\n",
    "      loss = train(model, optimizer, dat)\n",
    "      tmp_train_acc, tmp_val_acc, tmp_test_acc = test_model(model, dat)\n",
    "      best_time = opt['time']\n",
    "      if tmp_val_acc > val_acc:\n",
    "        best_epoch = epoch\n",
    "        train_acc = tmp_train_acc\n",
    "        val_acc = tmp_val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "        best_time = opt['time']\n",
    "      if not opt['no_early'] and model.odeblock.test_integrator.solver.best_val > val_acc:\n",
    "        best_epoch = epoch\n",
    "        val_acc = model.odeblock.test_integrator.solver.best_val\n",
    "        test_acc = model.odeblock.test_integrator.solver.best_test\n",
    "        train_acc = model.odeblock.test_integrator.solver.best_train\n",
    "        best_time = model.odeblock.test_integrator.solver.best_time\n",
    "      log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Best time: {:.4f}'\n",
    "      # print(log.format(epoch, time.time() - start_time, loss, model.fm.sum, model.bm.sum, train_acc, val_acc, test_acc, best_time))\n",
    "\n",
    "  print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d} and best time {:03f}'.format(val_acc, test_acc,\n",
    "                                                                                                     best_epoch,\n",
    "                                                                                                     best_time))\n",
    "  return model, dat, val_acc, tmp_test_acc, train_acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ShXeAjv-2kf"
   },
   "outputs": [],
   "source": [
    "def generate_and_delete_reconstructed_edges(dataset_name: str='Cora'):\n",
    "      # delete new predict and removed one\n",
    "    dataset, removed_edges, _ = prepare_data(dataset_name=dataset_name)\n",
    "    removed_edges_set = set(tuple(sorted((int(u), int(v)))) for u, v in zip(removed_edges[0], removed_edges[1]))\n",
    "    # dataset.data.edge_index = add_noisy_edges(dataset.data.edge_index, num_nodes=dataset.data.num_nodes, noise_ratio=0.001)\n",
    "    print(\"Deleted edges:\", len(removed_edges_set))\n",
    "    return dataset, removed_edges_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04DwucwkvD_v"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def score_edges(embeddings, edge_index):\n",
    "    u = embeddings[edge_index[0]]\n",
    "    v = embeddings[edge_index[1]]\n",
    "    return F.cosine_similarity(u, v)  # returns score per edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "416z0_BW9tpq"
   },
   "outputs": [],
   "source": [
    "def is_close_enough(embeddings, edge_index, epsilon=0.05):\n",
    "    u = embeddings[edge_index[0]]\n",
    "    v = embeddings[edge_index[1]]\n",
    "    distances = torch.norm(u - v, dim=1)\n",
    "    return distances <= epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBMED DATASET OP\n",
    "opt = {'M_nodes': 64, 'adaptive': False, 'add_source': True, 'adjoint': True, 'adjoint_method': 'adaptive_heun', \n",
    "       'adjoint_step_size': 1, 'alpha': 1.0, 'alpha_dim': 'sc', 'att_samp_pct': 1, 'attention_dim': 128, 'attention_norm_idx': 0,\n",
    "         'attention_rewiring': False, 'attention_type': 'cosine_sim', 'augment': False, 'baseline': False, 'batch_norm': False, \n",
    "         'beltrami': False, 'beta_dim': 'sc', 'block': 'rewire_attention', 'cpus': 1, 'data_norm': 'rw', 'dataset': 'Pubmed', \n",
    "         'decay': 0.0018236722171703636, 'directional_penalty': None, 'dropout': 0.07191100715473969, 'dt': 0.001, \n",
    "         'dt_min': 1e-05, 'epoch': 600, 'exact': False, 'fc_out': False, 'feat_hidden_dim': 64, \n",
    "         'function': 'laplacian', 'gdc_avg_degree': 64, 'gdc_k': 64, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk',\n",
    "           'gdc_threshold': 0.01, 'gpus': 1.0, 'grace_period': 20, 'heads': 1, 'heat_time': 3.0, 'hidden_dim': 64,\n",
    "             'input_dropout': 0.5, 'jacobian_norm2': None, 'kinetic_energy': None, 'label_rate': 0.5, 'leaky_relu_slope': 0.2, \n",
    "             'lr': 0.0095, 'max_epochs': 120, 'max_iters': 100, 'max_nfe': 5000, 'method': 'dopri5',\n",
    "               'metric': 'test_acc', 'mix_features': False, 'name': None, 'new_edges': 'random', 'no_alpha_sigmoid': False,\n",
    "                 'not_lcc': True, 'num_init': 1, 'num_samples': 400, 'num_splits': 8, 'ode_blocks': 1, 'optimizer': 'adamax', \n",
    "                 'patience': 50, 'pos_enc_dim': 'row', 'pos_enc_hidden_dim': 16, 'ppr_alpha': 0.05, 'reduction_factor': 10,\n",
    "                   'regularise': False, 'reweight_attention': False, 'rewire_KNN': False, 'rewire_KNN_T': 'T0', \n",
    "                   'rewire_KNN_epoch': 10, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False, 'rewiring': None, 'rw_addD': 0.02, \n",
    "                   'rw_rmvR': 0.02, 'self_loop_weight': 1, 'sparsify': 'S_hat', 'square_plus': True, 'step_size': 1,\n",
    "                     'threshold_type': 'addD_rvR', 'time': 12.942327880200853, 'tol_scale': 1991.0688305523001, \n",
    "                     'tol_scale_adjoint': 16324.368093998313, 'total_deriv': None, 'use_cora_defaults': False, 'use_flux': False, \n",
    "                     'use_labels': False, 'use_lcc': True, 'use_mlp': False, 'folder': 'pubmed_linear_att_beltrami_adj2', 'index': 0,\n",
    "                       'run_with_KNN': False, 'change_att_sim_type': False, 'reps': 1, 'max_test_steps': 100, 'no_early': True,\n",
    "                         'earlystopxT': 5.0, 'pos_enc_csv': False, 'pos_enc_type': 'GDC'}\n",
    "opt.update({\n",
    "    'lr': 0.005,\n",
    "    'optimizer': 'adam',\n",
    "    'hidden_dim': 128,\n",
    "    'feat_hidden_dim': 64,\n",
    "    'dropout': 0.3,\n",
    "    'input_dropout': 0.3,\n",
    "    'decay': 0.0005,\n",
    "    'max_epochs': 150,\n",
    "    'patience': 50,\n",
    "    'method': 'dopri5',\n",
    "    'max_nfe': 100000,\n",
    "    'tol_scale': 1e-3,\n",
    "    'tol_scale_adjoint': 1e-3,\n",
    "    'att_samp_pct': 1.0,\n",
    "    'rw_addD': 0.005,\n",
    "    'rw_rmvR': 0.005,\n",
    "    'heads': 4,\n",
    "    'use_labels': True,\n",
    "    'block': 'rewire_attention'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORA DATASET OPT\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "opt = {'self_loop_weight': 1, 'leaky_relu_slope': 0.2, 'heads': 2, 'K': 10, 'not_lcc': True, 'dataset': 'Cora', 'force_reload': True,\n",
    "        'attention_norm_idx': 0, 'simple': True, 'alpha': 0, 'alpha_dim': 'sc', 'beta_dim': 'sc', \"use_labels\": True,\n",
    "        'hidden_dim': 64, 'block': 'attention', 'function': 'laplacian', 'alpha_sigmoid': True, 'augment': False, 'adjoint': False,\n",
    "        'tol_scale': 70, 'time': 20, 'input_dropout': 0.5, 'dropout': 0.2, 'method': 'dopri5', 'optimizer':'adam', 'lr':0.009, \"use_mlp\": True,\n",
    "        'decay':0.007, 'epoch':20, 'kinetic_energy':None, 'jacobian_norm2':None, 'total_deriv':None, 'directional_penalty':None, \"beltrami\": False}\n",
    "opt[\"fc_out\"] = False\n",
    "opt[\"batch_norm\"] = False\n",
    "opt[\"heads\"] = 8\n",
    "opt[\"attention_dim\"] = 128\n",
    "opt['attention_type'] = 'scaled_dot'\n",
    "opt['label_rate'] = 0.5\n",
    "opt['square_plus'] = True\n",
    "opt['reweight_attention'] = False\n",
    "opt['step_size'] = 1\n",
    "opt['max_nfe'] = 5000\n",
    "opt['no_alpha_sigmoid'] = False\n",
    "opt['add_source'] = False\n",
    "opt['dataset'] = 'Cora'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITESEER DATASET OPT\n",
    "opt = {'M_nodes': 64, 'adaptive': False, 'add_source': True, 'adjoint': False, 'adjoint_method': 'adaptive_heun', \n",
    "       'adjoint_step_size': 1, 'alpha': 1.0, 'alpha_dim': 'sc', 'att_samp_pct': 1, 'attention_dim': 32, \n",
    "       'attention_norm_idx': 1, 'attention_rewiring': False, 'attention_type': 'exp_kernel', 'augment': False, \n",
    "       'baseline': False, 'batch_norm': False, 'beltrami': False, 'beta_dim': 'sc', 'block': 'attention', 'cpus': 1, \n",
    "       'data_norm': 'rw', 'dataset': 'Citeseer', 'decay': 0.1, 'directional_penalty': None, 'dropout': 0.7488085003122172, \n",
    "       'dt': 0.001, 'dt_min': 1e-05, 'epoch': 200, 'exact': True, 'fc_out': False, 'function': 'laplacian',\n",
    "       'gdc_avg_degree': 64, 'gdc_k': 128, 'gdc_method': 'ppr', 'gdc_sparsification': 'topk', 'gdc_threshold': 0.01, 'gpus': 1.0,\n",
    "       'grace_period': 20, 'heads': 8, 'heat_time': 3.0, 'hidden_dim': 80, 'input_dropout': 0.6803233752085334, 'jacobian_norm2': None, \n",
    "       'kinetic_energy': None, 'label_rate': 0.5, 'leaky_relu_slope': 0.5825086997804176, 'lr': 0.00863585231323069, 'max_epochs': 1000,\n",
    "       'max_iters': 100, 'max_nfe': 3000, 'method': 'dopri5', 'metric': 'accuracy', 'mix_features': False, 'name': 'Citeseer_beltrami_1_KNN',\n",
    "       'new_edges': 'random', 'no_alpha_sigmoid': False, 'not_lcc': True, 'num_class': 6, 'num_init': 2,\n",
    "       'num_nodes': 2120, 'num_samples': 400, 'num_splits': 1, 'ode_blocks': 1, 'optimizer': 'adam', 'patience': 100, \n",
    "       'pos_enc_dim': 'row', 'pos_enc_hidden_dim': 16, 'ppr_alpha': 0.05, 'reduction_factor': 4, 'regularise': False,\n",
    "       'reweight_attention': False, 'rewire_KNN': False, 'rewire_KNN_epoch': 10, 'rewire_KNN_k': 64, 'rewire_KNN_sym': False,\n",
    "       'rewiring': None, 'rw_addD': 0.02, 'rw_rmvR': 0.02, 'self_loop_weight': 1, 'sparsify': 'S_hat', 'square_plus': True,\n",
    "       'step_size': 1, 'threshold_type': 'addD_rvR', 'time': 7.874113442879092, 'tol_scale': 2.9010446330432815, 'tol_scale_adjoint': 1.0, \n",
    "       'total_deriv': None, 'use_cora_defaults': False, 'use_flux': False, 'use_labels': True, 'use_lcc': True, 'use_mlp': False, 'no_early': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "dataset, removed_edges = generate_and_delete_reconstructed_edges(dataset_name=opt[\"dataset\"])\n",
    "G = to_networkx(dataset.data)\n",
    "total_tests = 1  # Initial number of test rounds\n",
    "epsilon = 0.1     # Distance threshold for edge prediction\n",
    "known_edges = to_edge_set(dataset.x)\n",
    "\n",
    "reconstruction_counter = defaultdict(int)\n",
    "reconstructed_edges_list = []\n",
    "new_predicted_counter = defaultdict(int)\n",
    "\n",
    "num_samples = 50000\n",
    "all_possible = torch.combinations(torch.arange(G.number_of_nodes()), r=2)\n",
    "perm = torch.randperm(all_possible.size(0))[:num_samples]\n",
    "sampled_pairs = all_possible[perm]\n",
    "candidate_edge_index = sampled_pairs.T\n",
    "\n",
    "best_val_acc = test_acc = best_epoch = 0\n",
    "\n",
    "for i in range(total_tests):\n",
    "    dataset, removed_edges = generate_and_delete_reconstructed_edges(dataset_name=opt[\"dataset\"])\n",
    "    opt['num_feature'] = dataset.data.x.shape\n",
    "    model, dat, val_acc, tmp_test_acc, train_acc, loss = trainModel(opt, dataset)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "        best_epoch = i\n",
    "    log = 'Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(loss, model.fm.sum, model.bm.sum, train_acc, best_val_acc, test_acc))\n",
    "    print('best val accuracy {:03f} with test accuracy {:03f} at epoch {:d}'.format(best_val_acc, test_acc, best_epoch))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        feat = dat.x\n",
    "        feat = add_labels(feat, dat.y, dat.train_mask, model.num_classes, model.device)\n",
    "        node_embeddings = model(feat, None)\n",
    "        # Score all candidate edges (e.g., using cosine or dot-product)\n",
    "        scores = score_edges(node_embeddings, candidate_edge_index)\n",
    "\n",
    "        # (Optional) Score filtering - top 10% most confident scores\n",
    "        threshold_value = torch.quantile(scores, 0.90)\n",
    "        high_score_mask = scores >= threshold_value\n",
    "        high_score_edges = candidate_edge_index[:, high_score_mask]\n",
    "\n",
    "        # Distance-based filtering\n",
    "        # epsilon = torch.quantile(distances, 0.2).item()\n",
    "        # close_mask = distances <= epsilon\n",
    "\n",
    "        # predicted_edges = high_score_edges[:, close_mask]\n",
    "        distances = torch.norm(node_embeddings[high_score_edges[0]] - node_embeddings[high_score_edges[1]], dim=1)\n",
    "        epsilon = torch.quantile(distances, 0.3).item()\n",
    "        predicted_edges = high_score_edges[:, distances <= epsilon]\n",
    "\n",
    "\n",
    "        print(\"\\n\\nScore range:\", scores.min().item(), \"->\", scores.max().item())\n",
    "        print(\"Distance range:\", distances.min().item(), \"->\", distances.max().item())\n",
    "        print(\"Total predicted edges:\", predicted_edges.size(1))\n",
    "        # Compare predicted edges to removed edges\n",
    "        for u, v in zip(predicted_edges[0], predicted_edges[1]):\n",
    "            edge = tuple(sorted((int(u), int(v))))\n",
    "            if edge in removed_edges:\n",
    "                reconstruction_counter[edge] += 1\n",
    "                reconstructed_edges_list.append(edge)\n",
    "            else:\n",
    "                new_predicted_counter[edge] += 1\n",
    "\n",
    "        if len(reconstruction_counter) and len(new_predicted_counter):\n",
    "            print(f\"\\nRound {i + 1}, total removed reconstructed: {len(reconstruction_counter.keys())}, \"\n",
    "                  f\"total new reconstructed: {len(new_predicted_counter.keys())}\"\n",
    "                  f\"\\nTotal Weak Edges in round: {len(removed_edges)-len(reconstructed_edges_list)}\")\n",
    "        print(\"\\n ******************************************* \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFDWKsWHHewo"
   },
   "outputs": [],
   "source": [
    "reconstruction_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ozw7IQldBkaL"
   },
   "outputs": [],
   "source": [
    "scores = score_edges(node_embeddings, candidate_edge_index)\n",
    "top_k = 100\n",
    "topk_indices = scores.topk(top_k).indices\n",
    "predicted_edges = candidate_edge_index[:, topk_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYuIxgODG54t"
   },
   "outputs": [],
   "source": [
    "# Display the graph of reconstructed edges\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "new_labels = []\n",
    "new_counts = []\n",
    "recovered_labels = []\n",
    "recovered_counts = []\n",
    "\n",
    "for edge, count in reconstruction_counter.items():\n",
    "    label = f\"{edge[0]}-{edge[1]}\"\n",
    "    recovered_labels.append(label)\n",
    "    recovered_counts.append(count)\n",
    "\n",
    "for edge, count in new_predicted_counter.items():\n",
    "    label = f\"{edge[0]}-{edge[1]}\"\n",
    "    new_labels.append(label)\n",
    "    new_counts.append(count)\n",
    "# Plotly bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=recovered_labels,\n",
    "    y=recovered_counts,\n",
    "    name='Recovered (Originally Removed)',\n",
    "    marker_color='cornflowerblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=new_labels,\n",
    "    y=new_counts,\n",
    "    name='New Predicted (Not Originally Present)',\n",
    "    marker_color='orange'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Edge Reconstruction vs New Predictions\",\n",
    "    xaxis_title=\"Edge (u-v)\",\n",
    "    yaxis_title=\"Reconstruction Count\",\n",
    "    barmode='group',\n",
    "    xaxis_tickangle=-45,\n",
    "    template='plotly_white',\n",
    "    margin=dict(l=40, r=40, t=60, b=100)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4yhkTecodkp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary from the lists\n",
    "data = {\n",
    "    'Edge': recovered_labels + new_labels,\n",
    "    'Count': recovered_counts + new_counts,\n",
    "    'Type': ['Recovered'] * len(recovered_labels) + ['New Predicted'] * len(new_labels)\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('edge_reconstruction_data.csv', index=False)\n",
    "\n",
    "print(\"Data saved to edge_reconstruction_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvR0R1z2LGP1"
   },
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "# Create PyVis network\n",
    "net = Network(notebook=False, height=\"750px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "\n",
    "# Add graph from networkx\n",
    "net.from_nx(G)\n",
    "\n",
    "# Highlight reconstructed edges in red\n",
    "for u, v in reconstructed:\n",
    "    net.add_edge(int(u), int(v), color='red', width=10)\n",
    "\n",
    "# Optional: change node size or color\n",
    "for node in G.nodes():\n",
    "    net.get_node(node)['size'] = 10\n",
    "    net.get_node(node)['color'] = \"#cccccc\"\n",
    "\n",
    "# Save and open in browser\n",
    "net.save_graph(\"graph_reconstruction.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8NVMyaeZZbC"
   },
   "outputs": [],
   "source": [
    "total_data_to_review = [\n",
    "  {\n",
    "    \"iteration\": 1,\n",
    "    \"total_deleted\": 300,\n",
    "    \"total_reconstructed\": 180,\n",
    "    \"new_reconstructed\": 50\n",
    "  },\n",
    "  {\n",
    "    \"iteration\": 2,\n",
    "    \"total_deleted\": 300,\n",
    "    \"total_reconstructed\": 200,\n",
    "    \"new_reconstructed\": 45\n",
    "  },\n",
    "  {\n",
    "    \"iteration\": 3,\n",
    "    \"total_deleted\": 300,\n",
    "    \"total_reconstructed\": 190,\n",
    "    \"new_reconstructed\": 55\n",
    "  }\n",
    "]\n",
    "\n",
    "# Extract values\n",
    "iterations = [entry[\"iteration\"] for entry in total_data_to_review]\n",
    "deleted = [entry[\"total_deleted\"] for entry in total_data_to_review]\n",
    "reconstructed = [entry[\"total_reconstructed\"] for entry in total_data_to_review]\n",
    "new_reconstructed = [entry[\"new_reconstructed\"] for entry in total_data_to_review]\n",
    "\n",
    "# Bar chart setup\n",
    "bar_width = 0.25\n",
    "x = np.arange(len(iterations))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Bars\n",
    "plt.bar(x - bar_width, deleted, width=bar_width, label='Total Deleted')\n",
    "plt.bar(x, reconstructed, width=bar_width, label='Total Reconstructed')\n",
    "plt.bar(x + bar_width, new_reconstructed, width=bar_width, label='New Reconstructed')\n",
    "\n",
    "# Labels and ticks\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Number of Edges\")\n",
    "plt.title(\"Edge Reconstruction Metrics Per Iteration\")\n",
    "plt.xticks(x, [f\"Iter {i}\" for i in iterations])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuARNTWjukwJ"
   },
   "outputs": [],
   "source": [
    "# %80 %20 - accuracy best\n",
    "# %70 %30 - accuracy best\n",
    "# ....\n",
    "# 85% %15 - ....\n",
    "\n",
    "\n",
    "# removed edges -> evaluations\n",
    "# delete_reconstructed....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2b_h-mEpDBe"
   },
   "outputs": [],
   "source": [
    "def get_node_embeddings(model, data):\n",
    "    \"\"\"Extract final node embeddings from the GNN model\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # If use_labels is True, add labels to features before passing to the model\n",
    "        if model.opt['use_labels']:\n",
    "            x = add_labels(data.x, data.y, data.train_mask, model.num_classes, model.device)\n",
    "        else:\n",
    "            x = data.x\n",
    "\n",
    "        # Get model output, handle single or multiple outputs\n",
    "        output = model(x, None)\n",
    "\n",
    "        # If only embeddings are returned, unpack accordingly\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            embeddings = output\n",
    "        # If multiple outputs are returned, unpack as before\n",
    "        else:\n",
    "            embeddings, _ = output\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def calculate_confidence(embeddings, edge_index):\n",
    "    \"\"\"Calculate cosine similarity between connected nodes\"\"\"\n",
    "    src_emb = embeddings[edge_index[0]]\n",
    "    dst_emb = embeddings[edge_index[1]]\n",
    "    confidence = torch.cosine_similarity(src_emb, dst_emb).numpy()\n",
    "    return (edge_index.numpy(), confidence)\n",
    "\n",
    "def draw_confidence_graph(edges, confidence, threshold=0.5):\n",
    "    \"\"\"Draw graph with confidence-colored edges\"\"\"\n",
    "    G = nx.Graph()\n",
    "    edge_list = []\n",
    "\n",
    "    for i in range(edges.shape[1]):\n",
    "        if confidence[i] > threshold:\n",
    "            edge_list.append((edges[0,i], edges[1,i],\n",
    "                           {'confidence': confidence[i]}))\n",
    "\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    edge_colors = [d['confidence'] for _,_,d in G.edges(data=True)]\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    nx.draw(G, pos, node_size=20,\n",
    "            edge_color=edge_colors, edge_cmap=plt.cm.Reds,\n",
    "            width=1.5, arrows=False)\n",
    "    plt.title(\"Reconstructed Edges with Confidence Scores\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45SKdA-Fwh5v"
   },
   "outputs": [],
   "source": [
    "# After training the model\n",
    "embeddings = get_node_embeddings(model, dataset.data)\n",
    "edges, confidence = calculate_confidence(embeddings, dataset.data.edge_index)\n",
    "\n",
    "# Visualize high-confidence edges (threshold=0.7)\n",
    "draw_confidence_graph(edges, confidence, threshold=0.7)\n",
    "\n",
    "# Compare with original graph\n",
    "draw_graph_after_adding_noise(dataset, edges.T)  # Existing visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN-kE5vg3z5_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_node_embeddings(model, data):\n",
    "    \"\"\"Extract final node embeddings from the GNN model\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # If use_labels is True, add labels to features before passing to the model\n",
    "        if model.opt['use_labels']:\n",
    "            x = add_labels(data.x, data.y, data.train_mask, model.num_classes, model.device)\n",
    "        else:\n",
    "            x = data.x\n",
    "\n",
    "        # Get model output, handle single or multiple outputs\n",
    "        output = model(x, None)\n",
    "\n",
    "        # If only embeddings are returned, unpack accordingly\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            embeddings = output\n",
    "        # If multiple outputs are returned, unpack as before\n",
    "        else:\n",
    "            embeddings, _ = output\n",
    "    return embeddings\n",
    "\n",
    "def calculate_confidence(embeddings, edge_index):\n",
    "    \"\"\"Calculate cosine similarity between connected nodes\"\"\"\n",
    "    src_emb = embeddings[edge_index[0]]\n",
    "    dst_emb = embeddings[edge_index[1]]\n",
    "    scores = score_edges(node_embeddings, candidate_edge_index)  # Already used\n",
    "    top_scores, top_indices = scores.topk(100)\n",
    "    predicted_edges = candidate_edge_index[:, top_indices]\n",
    "    confidence = top_scores.cpu().numpy()\n",
    "    return (edge_index.numpy(), confidence)\n",
    "\n",
    "def draw_confidence_graph(edges, confidence, original_edge_index=None, threshold=None, figsize=(12, 16)):\n",
    "    \"\"\"\n",
    "    Draw graph with confidence-colored edges and show a matching histogram\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    edges : numpy.ndarray\n",
    "        Edge list with shape (2, num_edges) for reconstructed edges\n",
    "    confidence : numpy.ndarray\n",
    "        Confidence scores for each edge\n",
    "    original_edge_index : torch.Tensor or numpy.ndarray, optional\n",
    "        The original edge indices from the dataset\n",
    "    threshold : float or None\n",
    "        If provided, shows a line at this threshold in the histogram\n",
    "        and will be used as the minimum value for the colormap\n",
    "    figsize : tuple\n",
    "        Figure size for the plot (width, height)\n",
    "    \"\"\"\n",
    "    # Create figure with stacked subplots - graph on top (larger), histogram below\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize,\n",
    "                                  gridspec_kw={'height_ratios': [3, 1]})  # 3:1 ratio favoring the graph\n",
    "\n",
    "    # Build the graph with ALL confidence values (no filtering)\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Convert original edge indices to set for fast lookup if provided\n",
    "    original_edges_set = set()\n",
    "    if original_edge_index is not None:\n",
    "        if isinstance(original_edge_index, torch.Tensor):\n",
    "            original_edge_index = original_edge_index.numpy()\n",
    "\n",
    "        for i in range(original_edge_index.shape[1]):\n",
    "            # Add both directions (undirected graph)\n",
    "            edge = (original_edge_index[0, i], original_edge_index[1, i])\n",
    "            original_edges_set.add(edge)\n",
    "            original_edges_set.add(edge[::-1])  # Add reverse edge too\n",
    "\n",
    "    # Separate edges into original (existing) and reconstructed\n",
    "    original_edge_tuples = []\n",
    "    reconstructed_edge_tuples = []\n",
    "    reconstructed_confidence = []\n",
    "\n",
    "    if isinstance(edges, torch.Tensor):\n",
    "        edges = edges.numpy()\n",
    "\n",
    "\n",
    "    for i in range(edges.shape[1]):\n",
    "        u, v = edges[0, i], edges[1, i]\n",
    "        current_edge_tuple = tuple(sorted((int(u), int(v)))) # Ensure consistent edge representation\n",
    "\n",
    "        if original_edge_index is not None and current_edge_tuple in original_edges_set:\n",
    "            # This edge exists in the original graph\n",
    "            original_edge_tuples.append((u, v))\n",
    "        else:\n",
    "            # This is a reconstructed or potentially new edge\n",
    "            reconstructed_edge_tuples.append((u, v))\n",
    "            reconstructed_confidence.append(confidence[i])  # Append the confidence score\n",
    "\n",
    "    # Add all unique edges to the graph first\n",
    "    all_unique_edges = list(set(original_edge_tuples + reconstructed_edge_tuples))\n",
    "    G.add_edges_from(all_unique_edges)\n",
    "\n",
    "    # Add attributes to the reconstructed edges\n",
    "    for i, edge in enumerate(reconstructed_edge_tuples):\n",
    "        u, v = edge\n",
    "        G[u][v]['confidence'] = reconstructed_confidence[i]\n",
    "        G[u][v]['original'] = False # Explicitly mark as not original\n",
    "\n",
    "    # Add attribute to original edges (if needed for later use)\n",
    "    for u, v in original_edge_tuples:\n",
    "         # Check if the edge exists before adding attribute\n",
    "         if G.has_edge(u, v):\n",
    "             G[u][v]['original'] = True\n",
    "\n",
    "\n",
    "    # Set up color mapping for reconstructed edges\n",
    "    cmap = plt.cm.viridis  # Using viridis colormap (can be changed to Reds, etc.)\n",
    "\n",
    "    # Set colormap boundaries for reconstructed edges\n",
    "    if reconstructed_confidence:  # Only if we have reconstructed edges\n",
    "        if threshold is not None:\n",
    "            vmin = threshold\n",
    "        else:\n",
    "            vmin = min(reconstructed_confidence)\n",
    "        vmax = max(reconstructed_confidence)\n",
    "    else:\n",
    "        vmin, vmax = 0, 1  # Default if no reconstructed edges\n",
    "\n",
    "    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # Draw the graph on the top subplot (larger area)\n",
    "    pos = nx.spring_layout(G, seed=42)  # Fixed seed for reproducibility\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=2, node_color='gray', ax=ax1)\n",
    "\n",
    "    # Draw original edges in black\n",
    "    original_edges_to_draw = [(u, v) for u, v, d in G.edges(data=True) if d.get('original', False)]\n",
    "    if original_edges_to_draw:\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                             edgelist=original_edges_to_draw,\n",
    "                             edge_color='black',\n",
    "                             width=0.2,\n",
    "                             alpha=0.9,\n",
    "                             ax=ax1,\n",
    "                             label='Original Edges')\n",
    "\n",
    "    # Draw reconstructed edges with confidence colors\n",
    "    reconstructed_edges_to_draw = [(u, v) for u, v, d in G.edges(data=True) if not d.get('original', True) and 'confidence' in d]\n",
    "    if reconstructed_edges_to_draw:\n",
    "        edge_colors = [G[u][v]['confidence'] for u, v in reconstructed_edges_to_draw]\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                             edgelist=reconstructed_edges_to_draw,\n",
    "                             edge_color=edge_colors,\n",
    "                             edge_cmap=cmap,\n",
    "                             edge_vmin=vmin,\n",
    "                             edge_vmax=vmax,\n",
    "                             width=3.5,\n",
    "                             alpha=0.7,\n",
    "                             ax=ax1,\n",
    "                             label='Reconstructed Edges')\n",
    "\n",
    "        # Add a colorbar for edge confidence\n",
    "        sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=ax1, label='Confidence Score', orientation='vertical',\n",
    "                           pad=0.01, fraction=0.05)\n",
    "\n",
    "    ax1.set_title(\"Graph with Original (Black) and Reconstructed (Colored) Edges\", fontsize=14)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Add legend for edge types\n",
    "    # Check if both types of edges were drawn before adding legend\n",
    "    if original_edges_to_draw or reconstructed_edges_to_draw:\n",
    "        from matplotlib.lines import Line2D\n",
    "        legend_elements = []\n",
    "        if original_edges_to_draw:\n",
    "             legend_elements.append(Line2D([0], [0], color='black', lw=4, label='Original Edges'))\n",
    "        if reconstructed_edges_to_draw:\n",
    "             # Using cmap(0.5) gives an intermediate color from the colormap for the legend\n",
    "             legend_elements.append(Line2D([0], [0], color=cmap(0.5 if reconstructed_confidence else 0), lw=3.5, label='Reconstructed Edges'))\n",
    "        if legend_elements:\n",
    "            ax1.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "\n",
    "    # Draw the histogram on the bottom subplot for reconstructed edge confidence\n",
    "    if reconstructed_confidence:\n",
    "        bins = np.linspace(min(reconstructed_confidence), max(reconstructed_confidence), 20)\n",
    "        ax2.hist(reconstructed_confidence, bins=bins, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        ax2.set_title(\"Distribution of Reconstructed Edge Confidence Scores\", fontsize=12)\n",
    "        ax2.set_xlabel(\"Confidence Score\")\n",
    "        ax2.set_ylabel(\"Frequency\")\n",
    "        ax2.grid(alpha=0.3)\n",
    "\n",
    "        # Add vertical line for threshold if provided\n",
    "        if threshold is not None:\n",
    "            ax2.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.2f}') # Format threshold for clarity\n",
    "            ax2.legend()\n",
    "\n",
    "        # Add statistics to the histogram plot\n",
    "        avg_confidence = np.mean(reconstructed_confidence)\n",
    "        median_confidence = np.median(reconstructed_confidence)\n",
    "\n",
    "        # Add statistics to the histogram plot\n",
    "        avg_confidence = np.mean(reconstructed_confidence)\n",
    "        median_confidence = np.median(reconstructed_confidence)\n",
    "\n",
    "        stats_text = (f\"Statistics (Reconstructed Edges):\\n\"\n",
    "                     f\"Mean: {avg_confidence:.3f}\\n\"\n",
    "                     f\"Median: {median_confidence:.3f}\\n\"\n",
    "                     f\"Min: {min(reconstructed_confidence):.3f}\\n\"\n",
    "                     f\"Max: {max(reconstructed_confidence):.3f}\\n\"\n",
    "                     f\"Edges: {len(reconstructed_confidence)}\")\n",
    "\n",
    "        ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"No reconstructed edges to display\",\n",
    "                 transform=ax2.transAxes, ha='center', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0u9TT3Gn3059"
   },
   "outputs": [],
   "source": [
    "# After training the model\n",
    "embeddings = get_node_embeddings(model, dataset.data)\n",
    "edges, confidence = calculate_confidence(embeddings, dataset.data.edge_index)\n",
    "\n",
    "# Visualize with the new function including original edge information\n",
    "fig, (graph_ax, hist_ax) = draw_confidence_graph(\n",
    "    edges,\n",
    "    confidence,\n",
    "    original_edge_index=dataset.data.edge_index,  # Pass original edges\n",
    "    threshold=0.1\n",
    ")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
